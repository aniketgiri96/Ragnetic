# Configuration Guide

Ragnetic backend settings are read from environment variables (see `backend/app/core/config.py`).

## Core variables

| Variable | Default | Purpose |
|----------|---------|---------|
| `DATABASE_URL` | `postgresql://ragnetic:ragneticpassword@localhost:5432/ragnetic` | Postgres connection |
| `REDIS_URL` | `redis://localhost:6379/0` | Redis cache / queue base URL |
| `QDRANT_URL` | `http://localhost:6333` | Qdrant endpoint |
| `CELERY_BROKER_URL` | falls back to `REDIS_URL` | Celery broker |
| `CELERY_RESULT_BACKEND` | `REDIS_URL` with `/0` replaced to `/1` | Celery result backend |
| `MINIO_URL` | `http://localhost:9000` | MinIO endpoint |
| `MINIO_ACCESS_KEY` | `admin` | MinIO access key |
| `MINIO_SECRET_KEY` | `password` | MinIO secret key |
| `MINIO_BUCKET` | `ragnetic` | Bucket name for uploads |

## Auth and security

| Variable | Default | Purpose |
|----------|---------|---------|
| `JWT_SECRET` | `change-me-in-production` | JWT signing secret |
| `JWT_ALGORITHM` | `HS256` | JWT algorithm |
| `JWT_EXPIRE_HOURS` | `24` | Token lifetime |
| `ENVIRONMENT` | `development` | Set to `production` to enforce secure JWT secret check at startup |

For production, always set a strong unique `JWT_SECRET`. Startup now fails in `production` when `JWT_SECRET` remains default.

## LLM settings

| Variable | Default | Purpose |
|----------|---------|---------|
| `OLLAMA_URL` | `http://localhost:11434` | Ollama API URL |
| `OLLAMA_MODEL` | `llama3.2` | Default local chat model |
| `LLM_TIMEOUT_SECONDS` | `90` | Request timeout for LLM generation |
| `LLM_CONNECT_TIMEOUT_SECONDS` | `5` | Connect timeout to LLM provider |
| `LLM_MODEL_CHECK_TIMEOUT_SECONDS` | `3` | Timeout for Ollama model availability check |
| `OLLAMA_NUM_PREDICT` | `220` | Max tokens generated by Ollama request |
| `OLLAMA_TEMPERATURE` | `0.1` | Generation temperature for Ollama |
| `OPENAI_API_KEY` | empty | Optional OpenAI fallback |

If `OPENAI_API_KEY` is set and the OpenAI SDK is installed, chat uses OpenAI (`gpt-4o-mini`) first; otherwise it uses Ollama.

## Ingestion and chunking settings

| Variable | Default | Purpose |
|----------|---------|---------|
| `CHUNK_MAX_CHARS` | `600` | Max chunk size for ingestion |
| `CHUNK_OVERLAP_CHARS` | `80` | Overlap between adjacent chunks |
| `CHUNK_OVERLAP_SENTENCES` | `1` | Number of trailing sentences reused between adjacent chunks |
| `CHUNK_MIN_CHARS` | `180` | Minimum chunk size target before flushing |

## Chat context settings

| Variable | Default | Purpose |
|----------|---------|---------|
| `CHAT_CONTEXT_MAX_SOURCES` | `4` | Max retrieved source snippets inserted into prompt |
| `CHAT_CONTEXT_MAX_CHARS_PER_SOURCE` | `420` | Per-source snippet character limit in prompt |
| `CHAT_UNIQUE_SOURCES_PER_DOCUMENT` | `true` | Keep at most one retrieved chunk per document in chat context |
| `CHAT_MODEL_CONTEXT_TOKENS` | `8192` | Approximate model context window used for adaptive packing |
| `CHAT_CONTEXT_BUDGET_RATIO` | `0.75` | Fraction of model context allocated to retrieved evidence |
| `CHAT_CONTEXT_RESERVED_TOKENS` | `1200` | Tokens reserved for system prompt, user query, and response headroom |
| `CHAT_CONTEXT_MIN_TOKENS_PER_SOURCE` | `80` | Minimum token target for each included source |
| `CHAT_CONTEXT_MAX_TOKENS_PER_SOURCE` | `260` | Maximum token cap per included source after compression |
| `CHAT_CONTEXT_COMPRESSION_ENABLED` | `true` | Enable relevance-based snippet compression before prompt assembly |
| `CHAT_CONTEXT_COMPRESSION_TARGET_RATIO` | `0.60` | Compression target ratio relative to original snippet length |
| `CHAT_LOW_CONFIDENCE_THRESHOLD` | `0.45` | Marks chat answer as low-confidence when retrieval quality is below this score |
| `CHAT_ENFORCE_CITATION_FORMAT` | `true` | Appends `[Source N]` citations when model output omits them |
| `CHAT_ENABLE_FAITHFULNESS_SCORING` | `true` | Compute grounding faithfulness score for generated chat answers |
| `CHAT_FAITHFULNESS_THRESHOLD` | `0.55` | Marks answer as low-faithfulness when grounding score is below this threshold |

## Retrieval settings

| Variable | Default | Purpose |
|----------|---------|---------|
| `RETRIEVAL_TOP_K` | `5` | Number of chunks returned for search/chat context |
| `RETRIEVAL_DENSE_LIMIT` | `20` | Dense vector candidates pulled from Qdrant |
| `RETRIEVAL_SPARSE_POOL` | `240` | Max points scanned for lexical BM25 scoring |
| `RETRIEVAL_RERANK_TOP_N` | `8` | Number of fused candidates passed to optional rerank |
| `RETRIEVAL_ENABLE_CROSS_ENCODER` | `false` | Enable local cross-encoder reranking |
| `RETRIEVAL_ENABLE_QUERY_EXPANSION` | `true` | Expand each user query into multiple retrieval variants |
| `RETRIEVAL_QUERY_EXPANSION_MAX_VARIANTS` | `4` | Maximum number of query variants used for hybrid fusion |
| `RETRIEVAL_ENABLE_HYDE` | `false` | Enable LLM-generated HyDE synthetic passage as an additional variant |
| `RETRIEVAL_HYDE_MAX_CHARS` | `700` | Maximum HyDE synthetic passage length included in retrieval |

## Analytics and drift settings

| Variable | Default | Purpose |
|----------|---------|---------|
| `ANALYTICS_DEFAULT_WINDOW_DAYS` | `7` | Default analytics time window when `days` is not provided |
| `ANALYTICS_TOP_QUERIES_LIMIT` | `8` | Maximum number of top-query rows returned in analytics response |
| `ANALYTICS_DRIFT_ZERO_RESULT_RATE_THRESHOLD` | `0.30` | Triggers drift alert when zero-result query ratio exceeds this threshold |
| `ANALYTICS_DRIFT_RETRIEVAL_MS_THRESHOLD` | `1400` | Triggers drift alert when average retrieval latency exceeds this value |
| `ANALYTICS_DRIFT_LOW_FAITHFULNESS_RATE_THRESHOLD` | `0.30` | Triggers drift alert when low-faithfulness rate exceeds this threshold |
| `ANALYTICS_DRIFT_LOW_CONFIDENCE_RATE_THRESHOLD` | `0.35` | Triggers drift alert when low-confidence rate exceeds this threshold |

## Frontend variable

| Variable | Default | Purpose |
|----------|---------|---------|
| `NEXT_PUBLIC_API_URL` | `http://localhost:8000` | Frontend API base URL |

## Docker Compose defaults

`docker-compose.yml` already wires service-to-service URLs for local deployment. If you change ports/hosts, update both:

1. Backend environment values in `docker-compose.yml`.
2. Frontend `NEXT_PUBLIC_API_URL` in `docker-compose.yml` (or local env).

## Production notes

- Replace all default credentials (`POSTGRES_PASSWORD`, MinIO keys, JWT secret).
- Do not expose internal services publicly unless required (`redis`, `qdrant`, `db`).
- Run behind a reverse proxy with TLS.
- Pin model versions and keep embedding model consistent between indexing and querying.
- Update CORS origins in `backend/app/main.py` if frontend host is not `http://localhost:3000`.
